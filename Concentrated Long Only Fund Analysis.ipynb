{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651f2c16-bad6-44e3-8dc5-a22bd72cd51d",
   "metadata": {},
   "source": [
    "# Concentrated Long Only Fund Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18c97a6-7174-45dd-b853-afeeea018c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Stock Symbol  Type  Shares Held  Market Value  \\\n",
      "0                  Microsoft corp.   MSFT   NaN   12189646.0  1.694726e+09   \n",
      "1              Paypal holdings inc   PYPL   NaN   15402344.0  1.595529e+09   \n",
      "2      Estee lauder companies inc.     EL   NaN    6766747.0  1.346244e+09   \n",
      "3                    Stryker corp.    SYK   NaN    5945105.0  1.285926e+09   \n",
      "4  Philip morris international inc     PM   NaN   16928583.0  1.285387e+09   \n",
      "\n",
      "   % of Portfolio  Previous % of Portfolio  Ranking  Change in shares  \\\n",
      "0          8.6626                   8.4958      1.0           18512.0   \n",
      "1          8.1556                   9.1795      2.0           11316.0   \n",
      "2          6.8814                   6.3553      3.0          105962.0   \n",
      "3          6.5731                   6.2146      4.0          143665.0   \n",
      "4          6.5703                   6.2478      5.0         1660076.0   \n",
      "\n",
      "    % Change Change Type  % Ownership Qtr first owned                  sector  \\\n",
      "0   0.152098    addition     0.159646         Q1 2015  INFORMATION TECHNOLOGY   \n",
      "1   0.073523    addition     1.308966         Q3 2015          COMMUNICATIONS   \n",
      "2   1.590834    addition     1.875692         Q2 2016        CONSUMER STAPLES   \n",
      "3   2.476368    addition     1.589175         Q4 2012             HEALTH CARE   \n",
      "4  10.872550    addition     1.088067         Q4 2012        CONSUMER STAPLES   \n",
      "\n",
      "   source_type  source_date  Avg Price  \n",
      "0          NaN          NaN    51.2934  \n",
      "1          NaN          NaN    40.8461  \n",
      "2          NaN          NaN   114.9070  \n",
      "3          NaN          NaN   100.4001  \n",
      "4          NaN          NaN    88.0520  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the relative path to the 'Historical_Data_13F' folder\n",
    "directory = './Historical_Data_13F_Updated/concentrated_long_only_funds'\n",
    "\n",
    "# List all CSV files in the 'Historical_Data_13F' directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Check if we have any CSV files to process\n",
    "if csv_files:\n",
    "    # Build the full file path for the first CSV file\n",
    "    first_csv_path = os.path.join(directory, csv_files[0])\n",
    "\n",
    "    # Load the first CSV file into a DataFrame\n",
    "    df = pd.read_csv(first_csv_path)\n",
    "\n",
    "    # Print the first few rows of the DataFrame\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"No CSV files found in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43547f47-8645-4852-baaf-f0525d9e320f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2606a30-db30-40e7-9660-892fb6f9cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker  fundsmith-2019_q3  fundsmith-2021_q4  fundsmith-2019_q4  \\\n",
      "0   MSFT                  1                  1                  1   \n",
      "1   PYPL                  1                  1                  1   \n",
      "2     EL                  1                  1                  1   \n",
      "3    SYK                  1                  1                  1   \n",
      "4     PM                  1                  1                  1   \n",
      "\n",
      "   fundsmith-2021_q1  fundsmith-2021_q3  fundsmith-2021_q2  fundsmith-2020_q4  \\\n",
      "0                  1                  1                  1                  1   \n",
      "1                  1                  1                  1                  1   \n",
      "2                  1                  1                  1                  1   \n",
      "3                  1                  1                  1                  1   \n",
      "4                  1                  1                  1                  1   \n",
      "\n",
      "   fundsmith-2020_q2  fundsmith-2020_q3  fundsmith-2020_q1  \n",
      "0                  1                  1                  1  \n",
      "1                  1                  1                  1  \n",
      "2                  1                  1                  1  \n",
      "3                  1                  1                  1  \n",
      "4                  1                  1                  1  \n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing CSV files\n",
    "directory = './Historical_Data_13F_Updated/concentrated_long_only_funds'\n",
    "\n",
    "# Initialize an empty dictionary to store the ticker presence data\n",
    "ticker_presence = {}\n",
    "\n",
    "# Get all CSV files from the directory\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate over the files and extract unique tickers\n",
    "for csv_file in csv_files:\n",
    "    # Strip the '.csv' extension for use as DataFrame column names\n",
    "    clean_file_name = csv_file.replace('.csv', '')\n",
    "\n",
    "    file_path = os.path.join(directory, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    unique_tickers = df['Symbol'].unique()\n",
    "    \n",
    "    # Update the ticker presence for each file\n",
    "    for ticker in unique_tickers:\n",
    "        if ticker not in ticker_presence:\n",
    "            # Initialize a new entry in the dictionary with the cleaned file names\n",
    "            ticker_presence[ticker] = {clean_name: 0 for clean_name in [name.replace('.csv', '') for name in csv_files]}\n",
    "        # Mark the presence of the ticker in the current file\n",
    "        ticker_presence[ticker][clean_file_name] = 1\n",
    "\n",
    "# Create a DataFrame from the ticker presence dictionary\n",
    "one_hot_encoded_df = pd.DataFrame.from_dict(ticker_presence, orient='index')\n",
    "\n",
    "# Reset the index to get tickers as a column instead of an index\n",
    "one_hot_encoded_df.reset_index(inplace=True)\n",
    "one_hot_encoded_df.rename(columns={'index': 'Ticker'}, inplace=True)\n",
    "\n",
    "# Save the one_hot_encoded_df DataFrame to a CSV file\n",
    "one_hot_encoded_df.to_csv('concentrated_long_only_fund_encoded_data.csv', index=False)\n",
    "\n",
    "# Print the first few rows of the one-hot encoded DataFrame to verify\n",
    "print(one_hot_encoded_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9569d8d-7da2-4bb6-8247-c0c3359f3799",
   "metadata": {},
   "source": [
    "## Isolating Tickers to Pull Updated Historical Data\n",
    "\n",
    "This being a dynamic analysis with updating information each quarter, the holdings within the funds and the historical pricing data of each financial instrument changes. Because of this, we are going to pull a list of all the unique individual tickers and run them through whatever data provider, scraper, or API can give us historical pricing. Because I do not have access to a tool like Bloomberg for example, I am going to pull them from [Yahoo Finance](http://localhost:8888/lab/tree/Documents/Github/capital-markets/Yahoo%20Finance%20API%20Data%20Pull.ipynb), which you can access using the provided link.\n",
    "\n",
    "Once doing so, we are going to update that information, place them into the Historical Data folder, which we will then access, clean and create a new dataframe inside of this project to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e156197-53d8-42d8-bc90-ed9175887979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ticker\n",
      "0    MSFT\n",
      "1    PYPL\n",
      "2      EL\n",
      "3     SYK\n",
      "4      PM\n",
      "5    INTU\n",
      "6    IDXX\n",
      "7      FB\n",
      "8     WAT\n",
      "9     MKC\n",
      "10    PEP\n",
      "11      V\n",
      "12    ADP\n",
      "13    BDX\n",
      "14    JNJ\n",
      "15   BF.B\n",
      "16    EFX\n",
      "17   MASI\n",
      "18   VRSK\n",
      "19   SABR\n",
      "20   CHKP\n",
      "21   ANSS\n",
      "22   CGNX\n",
      "23   IPGP\n",
      "24   VRSN\n",
      "25   MSCI\n",
      "26    AOS\n",
      "27   PAYC\n",
      "28    MAR\n",
      "29    CHD\n",
      "30   MELI\n",
      "31    MMM\n",
      "32    CDK\n",
      "33    NKE\n",
      "34   AMZN\n",
      "35   SBUX\n",
      "36  GOOGL\n",
      "37   FTNT\n",
      "38   QLYS\n",
      "39   WING\n",
      "40    ROL\n",
      "41     HD\n",
      "42    ZTS\n",
      "43     PG\n",
      "44    TSM\n",
      "45    WNS\n",
      "46      G\n",
      "47     CL\n",
      "48     XP\n",
      "49    CLX\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the 'Ticker' column into a new DataFrame\n",
    "tickers_df = pd.DataFrame(one_hot_encoded_df['Ticker'])\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(tickers_df.head(60))\n",
    "\n",
    "# If you want to save this to a new CSV file:\n",
    "#tickers_df.to_csv('list_of_tickers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4890e9-ba54-4d4c-8837-a87cab5cd33b",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dafef947-4f76-4a85-9101-4acf25d357dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = one_hot_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2404861-869d-4465-af70-37c9a31f2f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ticker  Quarters_Held\n",
      "0    MSFT             10\n",
      "15   BF.B             10\n",
      "30   MELI             10\n",
      "27   PAYC             10\n",
      "26    AOS             10\n",
      "1    PYPL             10\n",
      "24   VRSN             10\n",
      "23   IPGP             10\n",
      "22   CGNX             10\n",
      "21   ANSS             10\n",
      "19   SABR             10\n",
      "18   VRSK             10\n",
      "17   MASI             10\n",
      "16    EFX             10\n",
      "25   MSCI             10\n",
      "14    JNJ             10\n",
      "2      EL             10\n",
      "12    ADP             10\n",
      "11      V             10\n",
      "10    PEP             10\n",
      "9     MKC             10\n",
      "8     WAT             10\n",
      "7      FB             10\n",
      "6    IDXX             10\n",
      "5    INTU             10\n",
      "4      PM             10\n",
      "3     SYK             10\n",
      "13    BDX              9\n",
      "28    MAR              9\n",
      "29    CHD              7\n",
      "48     XP              7\n",
      "20   CHKP              7\n",
      "33    NKE              7\n",
      "35   SBUX              7\n",
      "38   QLYS              6\n",
      "47     CL              5\n",
      "37   FTNT              5\n",
      "44    TSM              4\n",
      "43     PG              4\n",
      "42    ZTS              4\n",
      "41     HD              3\n",
      "49    CLX              3\n",
      "40    ROL              3\n",
      "39   WING              3\n",
      "45    WNS              2\n",
      "31    MMM              2\n",
      "32    CDK              2\n",
      "36  GOOGL              1\n",
      "46      G              1\n",
      "34   AMZN              1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add a new column that sums how many quarters each company is held\n",
    "df['Quarters_Held'] = df.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Sort the DataFrame by the 'Quarters_Held' column in descending order\n",
    "df_sorted = df.sort_values(by='Quarters_Held', ascending=False)\n",
    "\n",
    "# Extract just the 'Ticker' and 'Quarters_Held' columns for a clear ranking\n",
    "ranked_companies = df_sorted[['Ticker', 'Quarters_Held']]\n",
    "\n",
    "# Display or save the ranked companies list\n",
    "print(ranked_companies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a616ca39-4404-4af0-ae7a-080cb548a3b4",
   "metadata": {},
   "source": [
    "## Uploading Historical Pricing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29fc480e-c802-421e-b87d-1feb4630a5ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Historical_Data_Prices_Cleaned/PYPL.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m MSFT  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistorical_Data_Prices_Cleaned/MSFT.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m PYPL \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHistorical_Data_Prices_Cleaned/PYPL.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m EL \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistorical_Data_Prices_Cleaned/EL.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m SYK \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistorical_Data_Prices_Cleaned/SYK.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_environment_01/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_environment_01/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_environment_01/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_environment_01/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_environment_01/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Historical_Data_Prices_Cleaned/PYPL.csv'"
     ]
    }
   ],
   "source": [
    "MSFT  = pd.read_csv('Historical_Data_Prices_Cleaned/MSFT.csv')\n",
    "PYPL = pd.read_csv('Historical_Data_Prices_Cleaned/PYPL.csv')\n",
    "EL = pd.read_csv('Historical_Data_Prices_Cleaned/EL.csv')\n",
    "SYK = pd.read_csv('Historical_Data_Prices_Cleaned/SYK.csv')\n",
    "PM = pd.read_csv('Historical_Data_Prices_Cleaned/PM.csv')\n",
    "INTU = pd.read_csv('Historical_Data_Prices_Cleaned/INTU.csv')\n",
    "IDXX = pd.read_csv('Historical_Data_Prices_Cleaned/IDXX.csv')\n",
    "FB = pd.read_csv('Historical_Data_Prices_Cleaned/FB.csv')\n",
    "WAT = pd.read_csv('Historical_Data_Prices_Cleaned/WAT.csv')\n",
    "MKC = pd.read_csv('Historical_Data_Prices_Cleaned/MKC.csv')\n",
    "PEP = pd.read_csv('Historical_Data_Prices_Cleaned/PEP.csv')\n",
    "V = pd.read_csv('Historical_Data_Prices_Cleaned/V.csv')\n",
    "ADP = pd.read_csv('Historical_Data_Prices_Cleaned/ADP.csv')\n",
    "BDX = pd.read_csv('Historical_Data_Prices_Cleaned/BDX.csv')\n",
    "JNJ = pd.read_csv('Historical_Data_Prices_Cleaned/JNJ.csv')\n",
    "BF.B = pd.read_csv('Historical_Data_Prices_Cleaned/BF.B.csv')\n",
    "EFX = pd.read_csv('Historical_Data_Prices_Cleaned/EFX.csv')\n",
    "MASI = pd.read_csv('Historical_Data_Prices_Cleaned/MASI.csv')\n",
    "VRSK = pd.read_csv('Historical_Data_Prices_Cleaned/VRSK.csv')\n",
    "SABR = pd.read_csv('Historical_Data_Prices_Cleaned/SABR.csv')\n",
    "CHKP = pd.read_csv('Historical_Data_Prices_Cleaned/CHKP.csv')\n",
    "ANSS = pd.read_csv('Historical_Data_Prices_Cleaned/ANSS.csv')\n",
    "CGNX = pd.read_csv('Historical_Data_Prices_Cleaned/CGNX.csv')\n",
    "IPGP = pd.read_csv('Historical_Data_Prices_Cleaned/IPGP.csv')\n",
    "VRSN = pd.read_csv('Historical_Data_Prices_Cleaned/VRSN.csv')\n",
    "MSCI = pd.read_csv('Historical_Data_Prices_Cleaned/MSCI.csv')\n",
    "AOS = pd.read_csv('Historical_Data_Prices_Cleaned/AOS.csv')\n",
    "PAYC = pd.read_csv('Historical_Data_Prices_Cleaned/PAYC.csv')\n",
    "MAR = pd.read_csv('Historical_Data_Prices_Cleaned/MAR.csv')\n",
    "CHD = pd.read_csv('Historical_Data_Prices_Cleaned/CHD.csv')\n",
    "MELI = pd.read_csv('Historical_Data_Prices_Cleaned/MELI.csv')\n",
    "MMM = pd.read_csv('Historical_Data_Prices_Cleaned/MMM.csv')\n",
    "CDK = pd.read_csv('Historical_Data_Prices_Cleaned/CDK.csv')\n",
    "NKE = pd.read_csv('Historical_Data_Prices_Cleaned/NKE.csv')\n",
    "AMZN = pd.read_csv('Historical_Data_Prices_Cleaned/AMZN.csv')\n",
    "SBUX = pd.read_csv('Historical_Data_Prices_Cleaned/SBUX.csv')\n",
    "GOOGL = pd.read_csv('Historical_Data_Prices_Cleaned/GOOGL.csv')\n",
    "FTNT = pd.read_csv('Historical_Data_Prices_Cleaned/FTNT.csv')\n",
    "QLYS = pd.read_csv('Historical_Data_Prices_Cleaned/QLYS.csv')\n",
    "WING = pd.read_csv('Historical_Data_Prices_Cleaned/WING.csv')\n",
    "ROL = pd.read_csv('Historical_Data_Prices_Cleaned/ROL.csv')\n",
    "HD = pd.read_csv('Historical_Data_Prices_Cleaned/HD.csv')\n",
    "ZTS = pd.read_csv('Historical_Data_Prices_Cleaned/ZTS.csv')\n",
    "PG = pd.read_csv('Historical_Data_Prices_Cleaned/PG.csv')\n",
    "TSM = pd.read_csv('Historical_Data_Prices_Cleaned/TSM.csv')\n",
    "WNS = pd.read_csv('Historical_Data_Prices_Cleaned/WNS.csv')\n",
    "G = pd.read_csv('Historical_Data_Prices_Cleaned/G.csv')\n",
    "CL = pd.read_csv('Historical_Data_Prices_Cleaned/CL.csv')\n",
    "XP = pd.read_csv('Historical_Data_Prices_Cleaned/XP.csv')\n",
    "CLX = pd.read_csv('Historical_Data_Prices_Cleaned/CLX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f221e-d905-45ae-a1ce-e1df76f87559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7ff69-5d4b-4b89-9e83-59f5c1682283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b92d53-b8c1-4c3f-8547-1a44a7f4baaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb290d-5afa-4a84-92d2-c3a9b2563cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73d354-472f-4276-af8e-823681d23dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f6536-d883-4317-b8d9-8d26bd6ff246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0e844-8a21-485b-b2b4-d396cf6541ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d86cc-1bee-4489-88e8-4190dfef24c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b08b0-af04-4c38-90cc-6d48557aca4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0533c-b7f4-45ba-9579-f21595ad6ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896e584-7ae9-49d8-8dd9-5c02884d89c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
